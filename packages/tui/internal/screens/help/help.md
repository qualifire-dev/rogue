## ðŸ“– About Rogue
Rogue is a powerful tool designed to evaluate the *performance*, *compliance*, and *reliability* of AI agents. 
It pits a dynamic *EvaluatorAgent* against your agent using Google's *A2A protocol*,  
testing it with a range of scenarios to ensure it behaves exactly as intended.



### âœ¨ Key Features
- ðŸ”„ **Dynamic Scenario Generation**
- ðŸ‘€ **Live Evaluation Monitoring**
- ðŸ“Š **Comprehensive Reporting**
- ðŸ” **Multi-Faceted Testing**
- ðŸ¤– **Broad Model Support**



## âŒ¨ï¸  Keyboard Shortcuts
- **Ctrl+N** - New Evaluation  
- **Ctrl+L** - Configure LLMs  
- **Ctrl+E** - Scenario Editor  
- **Ctrl+I** - Interview Mode  
- **Ctrl+S** - Settings  
- **Ctrl+H** - Help  
- **Esc**    - Back/Cancel 


## ðŸ’¬ Slash Commands
- `/models`   - Configure LLM providers  
- `/editor`   - Open scenario editor  
- `/eval`     - Start new evaluation  
- `/help`     - Show this help screen  
- `/settings` - Edit settings  


## ðŸ§­ Navigation
Type **/** in the command input to see available commands with auto-completion.

## ðŸ”„ Evaluation Workflow
1. *Configure*          - Set up agent endpoint and authentication details
2. *Generate Scenarios* - Input business context to create test scenarios  
3. *Run & Evaluate*     - Start evaluation and watch live agent interactions
4. *View Report*        - Review detailed Markdown report with findings and recommendations

> **Rogue** â€” *Built by devs, for devs who test smarter.*






