## 📖 About Rogue
Rogue is a powerful tool designed to evaluate the *performance*, *compliance*, and *reliability* of AI agents. 
It pits a dynamic *EvaluatorAgent* against your agent using Google's *A2A protocol*,  
testing it with a range of scenarios to ensure it behaves exactly as intended.



### ✨ Key Features
- 🔄 **Dynamic Scenario Generation**
- 👀 **Live Evaluation Monitoring**
- 📊 **Comprehensive Reporting**
- 🔍 **Multi-Faceted Testing**
- 🤖 **Broad Model Support**



## ⌨️  Keyboard Shortcuts
- **Ctrl+N** - New Evaluation  
- **Ctrl+L** - Configure LLMs  
- **Ctrl+E** - Scenario Editor  
- **Ctrl+I** - Interview Mode  
- **Ctrl+S** - Settings  
- **Ctrl+H** - Help  
- **Esc**    - Back/Cancel 


## 💬 Slash Commands
- `/models`   - Configure LLM providers  
- `/editor`   - Open scenario editor  
- `/eval`     - Start new evaluation  
- `/help`     - Show this help screen  
- `/settings` - Edit settings  


## 🧭 Navigation
Type **/** in the command input to see available commands with auto-completion.

## 🔄 Evaluation Workflow
1. *Configure*          - Set up agent endpoint and authentication details
2. *Generate Scenarios* - Input business context to create test scenarios  
3. *Run & Evaluate*     - Start evaluation and watch live agent interactions
4. *View Report*        - Review detailed Markdown report with findings and recommendations

> **Rogue** — *Built by devs, for devs who test smarter.*






